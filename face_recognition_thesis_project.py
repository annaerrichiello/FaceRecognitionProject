# -*- coding: utf-8 -*-
"""Face Recognition Thesis Project (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g_TUDy9B8yPtM9YZJDDN_JQxLZmnAmqz
"""

import cv2
import pickle
import os

from google.colab import files
files.upload() #upload kaggle.json

!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!ls ~/.kaggle
!chmod 600 /root/.kaggle/kaggle.json



!kaggle datasets download -d danupnelson/14-celebrity-faces-dataset

!unzip /content/14-celebrity-faces-dataset.zip

import glob
data = glob.glob('/content/14-celebrity-faces-dataset/data/*/*/*.jpg')

#identities we have in our data
identities = []
for file in glob.glob('/content/14-celebrity-faces-dataset/data/*/*'):
        identity = os.path.splitext(os.path.basename(file))[0]
        identities.append(identity)

#drop duplicates 
list(set(identities))

#read images through skimage.io
import numpy as np
from skimage import io
array=[]
for j in range(len(data)):
  array.append(io.imread(data[j]))

#read images through cv2
array_cv=[]
for i in range(len(data)):
  array_cv.append(cv2.imread(data[i]))

#convert to gray scale (images read through cv2)
gray=[]
for i in range(len(array_cv)):
  gray.append(cv2.cvtColor(array_cv[i], cv2.COLOR_BGR2GRAY))

#convert to rgb using images read with skimage.io
rgb=[]
for i in range(len(array)):
  rgb.append(cv2.cvtColor(array[i], cv2.COLOR_BGR2RGB))

#convert to rgb using images read with cv2.read
rgb_1=[]
for i in range(len(array_cv)):
  rgb_1.append(cv2.cvtColor(array_cv[i], cv2.COLOR_BGR2RGB))

import sys
faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
imagePath = sys.argv[1]

casc = []
for c in range(len(gray)):
  casc.append(faceCascade.detectMultiScale(gray[c], 1.3, 5))

from google.colab.patches import cv2_imshow

#Face recognition applied to all the images in the dataset
images= []
filename = []
for i in range(len(casc)):
  for (x, y, w, h) in casc[i]:
      images.append(cv2.rectangle(array_cv[i],(x,y),(x+w,y+h),(255,0,0),2))
      filename.append(data[i])

#Function that allows us to see the image x in 'images'
for n in range(len(images)):
  def show(n):
    cv2_imshow(images[n])

show(100)
filename[100]

show(50)
filename[50]

#Face detection on one image
img = io.imread('/content/14-celebrity-faces-dataset/data/train/kate_beckinsale/321px-Kate_Beckinsale_2006.jpg')

gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

import sys
faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
imagePath = sys.argv[1]
# Create the haar cascade

# Detect faces in the image. The algo analyzes every block in the image
faces = faceCascade.detectMultiScale(gray, 1.3, 5)

for (x, y, w,h) in faces:
    img_1 = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) #This function returns 4 values: the x and y location of the rectangle, 
                                                           #and the rectangleâ€™s width and height (w , h)

from google.colab.patches import cv2_imshow

cv2_imshow(img_1)

!pip install dlib

!pip install face_recognition

##FACE RECOGNITION

import dlib

import face_recognition

import numpy as np

from PIL import Image
Image.open(data[11])

face_recognition.face_encodings(rgb_1[11])

enc=[]
for i in range(len(rgb_1)):
  enc.append(face_recognition.face_encodings(rgb_1[i]))

import numpy as np
encodings=[]
for i in range(len(enc)):
  encodings.append(np.array(enc[i]))

similar_faces = []
for i in range(len(encodings)):
 face_recog = face_recognition.compare_faces(encodings[i], encodings[10])
 similar_faces.append(face_recog)

type(similar_faces)

similar_faces

import pandas as pd

df = pd.DataFrame(similar_faces)

s = df.squeeze()

#there are some images in which more than one face is identified
s

list_similarfaces10  = [i for i, e in enumerate(s[0]) if e == [True]]

#How to open images
from PIL import Image
image_list = []
for filename in data:
  im=Image.open(filename)
  image_list.append(im)

sim_list= []
for i in list_similarfaces10:
  im=image_list[i]
  sim_list.append(im)

#esempio
sim_list[4]

list_similarfaces10

image = []
for i in list_similarfaces10:
   image.append(data[i])

images = [Image.open(x) for x in image]
widths, heights = zip(*(i.size for i in images))

total_width = sum(widths)
max_height = max(heights)

new_im = Image.new('RGB', (total_width, max_height))

x_offset = 0
for im in images:
  new_im.paste(im, (x_offset,0))
  x_offset += im.size[0]

new_im.save('test.jpg')

new_im

data[38]

similar_faces2 = []
for i in range(len(encodings)):
 face_recog = face_recognition.compare_faces(encodings[i], encodings[38])
 similar_faces2.append(face_recog)



df = pd.DataFrame(similar_faces2)

s2 = df.squeeze()

list_similarfaces  = [i for i, e in enumerate(s2[0]) if e == [True]]

sim_list2= []
for i in list_similarfaces:
  im=image_list[i]
  sim_list2.append(im)

image2 = []
for i in list_similarfaces:
   image2.append(data[i])

images = [Image.open(x) for x in image2]
widths, heights = zip(*(i.size for i in images))

total_width = sum(widths)
max_height = max(heights)

new_im = Image.new('RGB', (total_width, max_height))

x_offset = 0
for im in images:
  new_im.paste(im, (x_offset,0))
  x_offset += im.size[0]

new_im.save('test.jpg')

new_im

enco = [y for y in encodings if 0 not in y.shape]

import pandas as pd

from sklearn.metrics.pairwise import euclidean_distances
#example of distance computation between encodings of image 0 and encodings of image 1
euclidean_distances(enco[0], enco[1])

##
list1 = []
for i in range(len(enco)):
  for n in range(len(enco[i])):
    e = enco[i][n]
    list1.append(e)

list1 = pd.DataFrame([i[0] for i in enco])

from scipy.cluster.vq import whiten
scaled_data = whiten(list1.to_numpy())
pd.DataFrame(scaled_data).describe()

from scipy.cluster.hierarchy import fcluster, linkage

link = linkage(scaled_data, method = 'ward', metric = 'euclidean')

pd.DataFrame(scaled_data).describe()

import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram

dendrogram = dendrogram(link)

plt.axhline(c='red',linestyle='--', y=35) 
plt.show()

#for y=35, number of clusters=14

from sklearn.cluster import KMeans
kmeans=KMeans(n_clusters=14)

kmeans.fit(list1)

clusters_kmeans=list(zip(kmeans.fit_predict(list1), data))

dataframe_kmeans= pd.DataFrame(clusters_kmeans)
cluster_id= dataframe_kmeans.groupby(0).agg(lambda x: list(x))
cluster_id.values[1]

image_kmeans = []
for i in cluster_id.index:
   image_kmeans.append(cluster_id.values[i])

image_kmeans[0][0]

images = [Image.open(x) for x in image_kmeans[5][0]]
widths, heights = zip(*(i.size for i in images))

total_width = sum(widths)
max_height = max(heights)

new_im1 = Image.new('RGB', (total_width, max_height))

x_offset = 0
for im in images:
  new_im1.paste(im, (x_offset,0))
  x_offset += im.size[0]

new_im1.save('test1.jpg')

new_im1

images = [Image.open(x) for x in image_kmeans[1][0]]
widths, heights = zip(*(i.size for i in images))

total_width = sum(widths)
max_height = max(heights)

new_im2 = Image.new('RGB', (total_width, max_height))

x_offset = 0
for im in images:
  new_im2.paste(im, (x_offset,0))
  x_offset += im.size[0]

new_im2.save('test2.jpg')

new_im2

list1= np.array(list1)

clust=kmeans.fit_predict(list1)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure(figsize=(26,6))
ax = fig.add_subplot(131, projection='3d')
plt.scatter(list1[clust==2, 0], list1[clust==2, 1], s=100, c='red', label ='Cluster 3')
plt.scatter(list1[clust==13, 0], list1[clust==13, 1], s=100, c='blue', label ='Cluster 14')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure(figsize=(26,6))
ax = fig.add_subplot(131)
plt.scatter(list1[clust==2, 0], list1[clust==2, 1], s=100, c='red', label ='Cluster 3')
plt.scatter(list1[clust==13, 0], list1[clust==13, 1], s=100, c='blue', label ='Cluster 14')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure(figsize=(26,6))
ax = fig.add_subplot(131, projection = '3d')
ax.scatter(list1[clust==0, 0], list1[clust==0, 1], s=100, c='red', label ='Cluster 1')
ax.scatter(list1[clust==1, 0], list1[clust==1, 1], s=100, c='blue', label ='Cluster 2')
ax.scatter(list1[clust==2, 0], list1[clust==2, 1], s=100, c='green', label ='Cluster 3')
ax.scatter(list1[clust==3, 0], list1[clust==3, 1], s=100, c='cyan', label ='Cluster 4')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure(figsize=(26,6))
ax = fig.add_subplot(131, projection='3d')
ax.scatter(list1[clust==4, 0], list1[clust==4, 1], s=100, c='magenta', label ='Cluster 5')
ax.scatter(list1[clust==5, 0], list1[clust==5, 1], s=100, c='black', label ='Cluster 6')
ax.scatter(list1[clust==6, 0], list1[clust==6, 1], s=100, c='yellow', label ='Cluster 7')
ax.scatter(list1[clust==7, 0], list1[clust==7, 1], s=100, c='lightblue', label ='Cluster 8')
ax.scatter(list1[clust==8, 0], list1[clust==8, 1], s=100, c='orange', label ='Cluster 9')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure(figsize=(26,6))
ax = fig.add_subplot(131, projection = '3d')
ax.scatter(list1[clust==9, 0], list1[clust==9, 1], s=100, c='purple', label ='Cluster 10')
ax.scatter(list1[clust==10, 0], list1[clust==10, 1], s=100, c='yellow', label ='Cluster 11')
ax.scatter(list1[clust==11, 0], list1[clust==11, 1], s=100, c='lightblue', label ='Cluster 12')
ax.scatter(list1[clust==12, 0], list1[clust==12, 1], s=100, c='orange', label ='Cluster 13')
ax.scatter(list1[clust==13, 0], list1[clust==13, 1], s=100, c='darkblue', label ='Cluster 14')

#it is possible to compute euclidean distances and try to avoid FP

euclidean_dist = []
for i in range(len(enco)):
  euclid = euclidean_distances(enco[i], enco[i])
  euclidean_dist.append(euclid)

from scipy.spatial import distance
euclid = []
distances = []
for i in range(len(list1)):
  for e in range(len(list1)):
    dist = distance.euclidean(list1[i], list1[e])
    euclid.append([i, e, dist])
    distances.append(dist)

import pandas as pd

df_euclid = pd.DataFrame(euclid, columns=['id1', 'id2', 'euclidean_distances'])

df_euclid

rows = pd.DataFrame(list1)

merge = (pd.merge(left=df_euclid, right=rows[0], left_on='id1', right_on=rows.index))

merge2=(pd.merge(left=merge, right=rows, left_on='id2', right_on=rows.index))

# getting Difference
merge2['Score_diff'] = abs(merge2['0_x'] - merge2['0_y'])

merge2['Score_diff'].mean()/2

import numpy as np

merge2['assumption'] = np.where((merge2['Score_diff'] < merge2['Score_diff'].mean()/2),1,0)

y = merge2['assumption']

X = merge2.drop('assumption', axis='columns')

array = np.array(merge2['euclidean_distances'])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))

pred=logreg.predict(X)

fpr1, tpr1, thresholds1 = roc_curve(pred, array)

import sys
np.set_printoptions(threshold=sys.maxsize)
thresholds1

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn import metrics

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
logit_roc_auc1 = roc_auc_score(y_test, logreg.predict(X_test))
fpr1, tpr1, thresholds1 = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr1, tpr1, label='Logistic Regression (area = %0.2f)' % logit_roc_auc1)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

df_id = X.groupby('id1').agg(lambda x: list(x))

lists = []
for i in range(len(df_id['euclidean_distances'])):
  d = df_id['euclidean_distances'][i]
  list_=[index for index,value in enumerate(d) if value <= 0.4013106]
  lists.append(list_)

l=lists

out = []
while len(l)>0:
    first, *rest = l
    first = set(first)

    lf = -1
    while len(first)>lf:
        lf = len(first)

        rest2 = []
        for r in rest:
            if len(first.intersection(set(r)))>0:
                first |= set(r)
            else:
                rest2.append(r)     
        rest = rest2

    out.append(first)
    l = rest

print(out)

for i in out:
    if(len(i) <= 2):
        out.remove(i)

len(out)

v1 = []
for v in out:
    if v != []:
        v1.append(v)

#example of grouping based on distance
x = []
for i in v1[11]:
  im=data[i]
  x.append(im)

images = [Image.open(x) for x in x]
widths, heights = zip(*(i.size for i in images))

total_width = sum(widths)
max_height = max(heights)

new_im4 = Image.new('RGB', (total_width, max_height))

x_offset = 0
for im in images:
  new_im4.paste(im, (x_offset,0))
  x_offset += im.size[0]

new_im4.save('test4.jpg')

new_im4

#example of grouping based on distance
image = []
for i in v1[4]: #4
    image.append(data[i])

images = [Image.open(x) for x in image]
widths, heights = zip(*(i.size for i in images))

total_width = sum(widths)
max_height = max(heights)

new_im5 = Image.new('RGB', (total_width, max_height))

x_offset = 0
for im in images:
  new_im5.paste(im, (x_offset,0))
  x_offset += im.size[0]

new_im5.save('test5.jpg')

new_im5

#Mahalanobis distance

enco_ = [i[0] for i in enco]

from scipy.spatial import distance

cov = np.cov(enco_, rowvar=False)
inv = np.linalg.inv(cov)

mahal = []
mahalanobis_dist = []
for i in range(len(list1)):
  for e in range(len(list1)):
    dist = distance.mahalanobis(list1[i], list1[e], inv)
    mahal.append([i, e, dist])
    mahalanobis_dist.append(dist)

df= pd.DataFrame(inv)
centerpoint = np.mean(enco_, axis=0)

from scipy.stats import chi2

distances = []
for i, val in enumerate(df):
      p1 = val
      p2 = centerpoint
      distance = (p1-p2).T.dot(df).dot(p1-p2)
      distances.append(distance)
distances = np.array(distances)

# Cutoff (threshold) value from Chi-Sqaure Distribution for detecting outliers 
cutoff = chi2.ppf(0.95, df.shape[1])

# Index of outliers
outlierIndexes = np.where(distances > cutoff )

print('Index of Outliers')
print(outlierIndexes)

print('Observations found as outlier')
print(df[ distances > cutoff])

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from matplotlib.patches import Ellipse
## Finding ellipse dimensions 
pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])
ell_radius_x = np.sqrt(1 + pearson)
ell_radius_y = np.sqrt(1 - pearson)
lambda_, v = np.linalg.eig(cov)
lambda_ = np.sqrt(lambda_)

# Ellipse patch
ellipse = Ellipse(xy=(centerpoint[0], centerpoint[1]),
                  width=lambda_[0]*np.sqrt(cutoff)*2, height=lambda_[1]*np.sqrt(cutoff)*2,
                  angle=np.rad2deg(np.arccos(v[0, 0])), edgecolor='#fab1a0')
ellipse.set_facecolor('#0984e3')
ellipse.set_alpha(0.5)
fig = plt.figure()
ax = plt.subplot()
ax.add_artist(ellipse)
plt.scatter(df[0], df[1])
plt.show()

df = pd.DataFrame(mahal, columns=['id1', 'id2', 'mahalanobis_distances'])

merge = (pd.merge(left=df, right=rows[0], left_on='id1', right_on=rows.index))

merge_mahal=(pd.merge(left=merge, right=rows, left_on='id2', right_on=rows.index))

merge_mahal['Score_diff'] = abs(merge_mahal['0_x'] - merge_mahal['0_y'])

merge_mahal['assumption'] = np.where((merge_mahal['Score_diff'] < merge_mahal ['Score_diff'].mean()/2), 1, 0)

y = merge_mahal['assumption']

X = merge_mahal.drop('assumption',axis='columns')

array = np.array(X['mahalanobis_distances'])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn import metrics

pred = logreg.predict(X)

fpr, tpr, thresholds = roc_curve(pred, array)

thresholds

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr1, tpr1, label='Logistic Regression for Euclidean Distances (area = %0.2f)' % logit_roc_auc1)
plt.plot(fpr, tpr, label='Logistic Regression for Mahalanobis Distances (area = %0.2f)' % logit_roc_auc) 
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curves')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

lists_ = []
for i in range(len(df_id_mahal['mahalanobis_distances'])):
  d = df_id_mahal['mahalanobis_distances'][i]
  list_=[index for index,value in enumerate(d) if value <= 12.67074448]
  lists_.append(list_)

l = lists_

out = []
while len(l)>0:
    first, *rest = l
    first = set(first)

    lf = -1
    while len(first)>lf:
        lf = len(first)

        rest2 = []
        for r in rest:
            if len(first.intersection(set(r)))>0:
                first |= set(r)
            else:
                rest2.append(r)     
        rest = rest2

    out.append(first)
    l = rest

print(out)

for i in out:
    if(len(i) == 1):
        out.remove(i)

len(out)

out

v2 = []
for v in out:
    if v != []:
        v2.append(v)

#example of grouping based on distance
x = []
for i in v2[3]:
  im=data[i]
  x.append(im)

images = [Image.open(x) for x in x]
widths, heights = zip(*(i.size for i in images))

total_width = sum(widths)
max_height = max(heights)

new_im6 = Image.new('RGB', (total_width, max_height))

x_offset = 0
for im in images:
  new_im6.paste(im, (x_offset,0))
  x_offset += im.size[0]

new_im6.save('test6.jpg')

new_im6